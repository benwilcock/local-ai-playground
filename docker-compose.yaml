version: '3.6'

## See also .env file for additional settings
name: local-ai-chatbot-ui-demo

services:
  ## OpenAI compatible API server capable of using several AI models
  ## including gpt4app-j (chat), stable diffusion (images), and whisper (audio)
  local-ai-api:
    container_name: local-ai-api
    image: quay.io/go-skynet/local-ai:v1.20.1 # was: quay.io/go-skynet/local-ai:v1.18.0
    # As initially LocalAI will download the models defined in PRELOAD_MODELS
    # you might need to tweak the healthcheck values here according to your network connection.
    # Here we give a timespan of 20m to download all the required files.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 1m
      timeout: 20m
      retries: 20
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 8080:8080 # Rest API will be on this port (outside:inside)
    environment:
      - 'THREADS=4' # Set this to the number of physical CPU's not threads
      - 'CONTEXT_SIZE=4096' # The historic memory for conversations
      - 'MODELS_PATH=/models' # Location of the models folder in the image
      - 'DEBUG=true' # More info for builds and output when tailing logs
      - 'GALLERIES=[{"name":"model-gallery", "url":"github:go-skynet/model-gallery/index.yaml"},{"url": "github:ci-robbot/localai-huggingface-zoo/index.yaml","name":"huggingface"}]'
      - 'REBUILD=true' # Required for Stable Diffusion mode
      - 'GO_TAGS=stablediffusion' # Required for Stable Diffusion mode
      - 'IMAGE_PATH=/tmp' # Required for Stable Diffusion mode
      - 'UPLOAD_LIMIT=100' # used by Whisper mode to stop massive uploads
      # You can preload (auto-download) different models. See: https://github.com/go-skynet/model-gallery
      # - 'PRELOAD_MODELS=[{"url": "github:go-skynet/model-gallery/stablediffusion.yaml"}]'
      # - 'PRELOAD_MODELS=[{"url": "github:go-skynet/model-gallery/gpt4all-j.yaml", "name": "gpt-3.5-turbo"}, {"url": "github:go-skynet/model-gallery/stablediffusion.yaml"}]'
    volumes:
      - ./models:/models:cached # Maps the model folder to this project's models folder
      - ./tmp:/tmp # Required for Stable Diffusion mode
      - ./audio:/audio # Required for Whisper
    command: ["/usr/bin/local-ai" ]

  # ChatBot UI - ChatGPT Like Frontend ######################
  chatbot-ui:
    container_name: chatbot-ui
    depends_on:
      local-ai-api:
        condition: service_healthy # Waits for healthcheck to pass before starting
    image: ghcr.io/mckaywrigley/chatbot-ui:main
    ports:
      - 3001:3000 # ChatBot UI will be on this port (outside:inside)
    environment:
      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'
      - 'OPENAI_API_HOST=http://local-ai-api:8080' # The LocalAI API endpoint
    # - 'NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT=### Instruction: {{.input}} ### Response:'
    # - 'NEXT_PUBLIC_DEFAULT_TEMPERATURE=0.8'
    # - 'DEFAULT_MODEL=gpt-3.5-turbo' # Specify the default model to use ("gpt-3.5-turbo" is default)

