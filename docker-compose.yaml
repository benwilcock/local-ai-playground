version: '3.6'

## See also .env file for additional settings
name: local-ai-chatbot

services:
  ## OpenAI compatible API server capable of using several AI models
  ## including gpt4app-j (chat), stable diffusion (images), and whisper (audio)
  api:
    container_name: api
    image: quay.io/go-skynet/local-ai:v1.18.0
    # As initially LocalAI will download the models defined in PRELOAD_MODELS
    # you might need to tweak the healthcheck values here according to your network connection.
    # Here we give a timespan of 20m to download all the required files.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 1m
      timeout: 20m
      retries: 20
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 8080:8080 # Rest API will be on this port (outside:inside)
    environment:
      - 'THREADS=4' # Set this to the number of physical CPU's not threads
      - 'CONTEXT_SIZE=1024' # The historic memory for conversations
      - 'MODELS_PATH=/models' # Location of the models folder in the image
      - 'DEBUG=true' # More info for builds and output when tailing logs
      # - 'REBUILD=true' # Required for Stable Diffusion mode
      # - 'GO_TAGS=stablediffusion' # Required for Stable Diffusion mode
      # - 'IMAGE_PATH=/tmp' # Required for Stable Diffusion mode
      # You can preload (auto-download) different models. See: https://github.com/go-skynet/model-gallery
      # - 'PRELOAD_MODELS=[{"url": "github:go-skynet/model-gallery/stablediffusion.yaml"}]'
      # - 'PRELOAD_MODELS=[{"url": "github:go-skynet/model-gallery/gpt4all-j.yaml", "name": "gpt-3.5-turbo"}, {"url": "github:go-skynet/model-gallery/stablediffusion.yaml"}]'
    volumes:
      - ./models:/models:cached # Maps the model folder to this project's models folder
      - ./tmp:/tmp # Required for Stable Diffusion mode
    command: ["/usr/bin/local-ai" ]

  ## ChatBot UI - ChatGPT Like Frontend ######################
  # chatgpt:
  #   container_name: chatgpt
  #   depends_on:
  #     api:
  #       condition: service_healthy # Waits for healthcheck to pass before starting
  #   image: ghcr.io/mckaywrigley/chatbot-ui:main
  #   ports:
  #     - 3000:3000 # ChatBot UI will be on this port (outside:inside)
  #   environment:
  #     - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'
  #     - 'OPENAI_API_HOST=http://api:8080' # Links to API container above
  #     - 'DEFAULT_MODEL=open-llama-7B-open-instruct' # Specify the default model to use
#      - 'DEFAULT_MODEL=ggml-gpt4all-j' # Specify the default model to use

  ## Alternate (Simple) Frontend ######################
  # frontend:
  #   image: quay.io/go-skynet/localai-frontend:master
  #   ports:
  #     - 3001:3000
