version: '3.6'

## See also .env file for additional settings
name: local-ai-chatbot-ui-demo

services:
  # OpenAI compatible API server capable of using several AI models including gpt4app-j (chat), stable diffusion (images), and whisper (audio)
  local-ai-api:
    container_name: local-ai-api
    # See https://quay.io/repository/go-skynet/local-ai?tab=tags for versions.
    image: quay.io/go-skynet/local-ai:v1.25.0 # was: quay.io/go-skynet/local-ai:v1.20.1
    # As initially LocalAI will download the models defined in PRELOAD_MODELS
    # you might need to tweak the healthcheck values here according to your network connection.
    # Here we give a timespan of 20m to download all the required files.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/readyz"]
      interval: 1m
      timeout: 20m
      retries: 20
    build:
      context: .
      dockerfile: Dockerfile
    environment:
      - 'THREADS=4' # Set this to the number of physical CPU's not threads
      - 'CONTEXT_SIZE=4096' # The historic memory for conversations
      - 'MODELS_PATH=/models' # Location of the models folder in the image
      - 'DEBUG=true' # More info for builds and output when tailing logs
      - 'GALLERIES=[{"name":"model-gallery", "url":"github:go-skynet/model-gallery/index.yaml"}, {"url": "github:go-skynet/model-gallery/huggingface.yaml","name":"huggingface"}]'
      #- 'REBUILD=true' # Required for Stable Diffusion mode
      # - 'GO_TAGS=stablediffusion' # Required for Stable Diffusion mode
      # - 'IMAGE_PATH=/tmp' # Required for Stable Diffusion mode
      # - 'UPLOAD_LIMIT=100' # used by Whisper mode to stop massive uploads
      # You can preload (auto-download) different models. See: https://github.com/go-skynet/model-gallery
      # - 'PRELOAD_MODELS=[{"url": "github:go-skynet/model-gallery/stablediffusion.yaml"}]'
      # - 'PRELOAD_MODELS=[{"url": "github:go-skynet/model-gallery/gpt4all-j.yaml", "name": "gpt-3.5-turbo"}, {"url": "github:go-skynet/model-gallery/stablediffusion.yaml"}]'
    volumes:
      - ./models:/models:cached # Maps the model folder to this project's models folder
      - ./tmp:/tmp # Required for Stable Diffusion mode
      - ./audio:/audio # Required for Whisper
    ports:
      - 8080:8080 # Rest API will be on this port (outside:inside)
    command: ["/usr/bin/local-ai" ]

  # ChatBot UI - ChatGPT Like Frontend ######################
  chatbot-ui:
    container_name: chatbot-ui
    image: ghcr.io/mckaywrigley/chatbot-ui:main
    environment:
      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'
      - 'OPENAI_API_HOST=http://local-ai-api:8080' # The LocalAI API endpoint
      # - 'OPENAI_API_KEY=sk-hknuewz5we7u8ez4mi78khgfc3ch4iq0'
      # - 'OPENAI_API_HOST=https://mockgpt.wiremockapi.cloud'
      # - 'NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT=You are a professional software programmer. Below is an instruction or question which describes a task or query. Write a response that appropriately completes the task or answers the query. Follow the users instructions carefully. Respond using markdown where appropriate. Be as concise as possible. ### Instruction: {{.Input}} ### Response:'
      - 'NEXT_PUBLIC_DEFAULT_TEMPERATURE=0.7'
      - 'DEFAULT_MODEL=gpt-3.5-turbo' # Specify the default model to use ("gpt-3.5-turbo" is default)
    depends_on:
      local-ai-api:
        condition: service_healthy # Waits for healthcheck to pass before starting
    ports:
      - 3001:3000 # ChatBot UI will be on this port (outside:inside)

  flowise:
    image: flowiseai/flowise:latest
    container_name: flowise
    restart: always
    environment:
        - PORT=3000
        - FLOWISE_USERNAME=user
        - FLOWISE_PASSWORD=password
        # - PASSPHRASE=MYPASSPHRASE # Passphrase used to create encryption key
        - DATABASE_PATH=/root/.flowise
        - APIKEY_PATH=/root/.flowise
        - SECRETKEY_PATH=/root/.flowise
        - LOG_PATH=/root/.flowise/logs
        - DEBUG=true
        - LOG_LEVEL=debug
    depends_on:
      local-ai-api:
        condition: service_healthy # Waits for healthcheck to pass before starting
    ports:
        - '3003:3000'
    volumes:
        - ./flowise:/root/.flowise
    command: /bin/sh -c "sleep 3; flowise start"

  # chroma: # https://github.com/chroma-core/chroma The OSS persistent embeddings database
  #   image: ghcr.io/chroma-core/chroma:latest
  #   container_name: chroma
  #   depends_on:
  #     local-ai-api:
  #       condition: service_healthy # Waits for healthcheck to pass before starting
  #   ports:
  #     - '3002:8000'
  #   volumes:
  #     - ./chroma_index:/chroma/.chroma/index

# volumes:
#   chroma_index:
#     driver: local
#   backups:
#     driver: local